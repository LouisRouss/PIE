{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce notebook explore différents modèle de machine learning pour du sentiment analysis, le texte est ici prétraité avec un BOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://ieeexplore.ieee.org/abstract/document/9142175"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import TweetDataFrame as TDF\n",
    "import Natural_Language_Processing as NLP\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import tweepy as tw\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le dataset comporte des textes orientés finance avec pour chaque texte une labelisation : positif, negatif ou neutre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>According to Gran , the company has no plans t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Technopolis plans to develop in stages an area...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>negative</td>\n",
       "      <td>The international electronic industry company ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>positive</td>\n",
       "      <td>With the new production plant the company woul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>positive</td>\n",
       "      <td>According to the company 's updated strategy f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4841</th>\n",
       "      <td>negative</td>\n",
       "      <td>LONDON MarketWatch -- Share prices ended lower...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4842</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Rinkuskiai 's beer sales fell by 6.5 per cent ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4843</th>\n",
       "      <td>negative</td>\n",
       "      <td>Operating profit fell to EUR 35.4 mn from EUR ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4844</th>\n",
       "      <td>negative</td>\n",
       "      <td>Net sales of the Paper segment decreased to EU...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4845</th>\n",
       "      <td>negative</td>\n",
       "      <td>Sales in Finland decreased by 10.5 % in Januar...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4846 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sentiment                                               text\n",
       "0      neutral  According to Gran , the company has no plans t...\n",
       "1      neutral  Technopolis plans to develop in stages an area...\n",
       "2     negative  The international electronic industry company ...\n",
       "3     positive  With the new production plant the company woul...\n",
       "4     positive  According to the company 's updated strategy f...\n",
       "...        ...                                                ...\n",
       "4841  negative  LONDON MarketWatch -- Share prices ended lower...\n",
       "4842   neutral  Rinkuskiai 's beer sales fell by 6.5 per cent ...\n",
       "4843  negative  Operating profit fell to EUR 35.4 mn from EUR ...\n",
       "4844  negative  Net sales of the Paper segment decreased to EU...\n",
       "4845  negative  Sales in Finland decreased by 10.5 % in Januar...\n",
       "\n",
       "[4846 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('dataset/dataset_sentiment.csv',sep=',',encoding='latin-1',header=None,names=['sentiment','text'])\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thus the group 's balance sheet will have about EUR25 .8 m of goodwill , the company added . neutral\n"
     ]
    }
   ],
   "source": [
    "## Rerun for another example\n",
    "ind = random.randint(0,dataset.shape[0]-1)\n",
    "print(dataset['text'].loc[ind],dataset['sentiment'].loc[ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On sépare le dataset en X et y. On encode ensuite le terme neutre par 0, positif par 1 et negatif par -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = OrdinalEncoder()\n",
    "X = dataset[['text']]\n",
    "y = dataset[['sentiment']]\n",
    "y = enc.fit_transform(y)\n",
    "y = y-1 # neutral = 0, positive = 1, negative = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.  0.  1.] [ 604 2879 1363]\n"
     ]
    }
   ],
   "source": [
    "unique, counts = np.unique(y, return_counts=True)\n",
    "print(unique,counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous constatons que les différentes classes sont déséquilibrées. Nous prenons donc 600 textes positifs, 600 textes negatifs et 600 textes neutres aléatoires pour pouvoir entrainer notre modèle sur un dataset equilibré"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train =[]\n",
    "y_train =[]\n",
    "k=0\n",
    "i=0\n",
    "while k<600:\n",
    "    if y[i]==-1:\n",
    "        X_train.append(X['text'][i])\n",
    "        y_train.append(y[i])\n",
    "        k+=1\n",
    "    i+=1\n",
    "k=0\n",
    "i=0\n",
    "while k<600:\n",
    "    if y[i]==0:\n",
    "        X_train.append(X['text'][i])\n",
    "        y_train.append(y[i])\n",
    "        k+=1\n",
    "    i+=1\n",
    "k=0\n",
    "i=0\n",
    "while k<600:\n",
    "    if y[i]==1:\n",
    "        X_train.append(X['text'][i])\n",
    "        y_train.append(y[i])\n",
    "        k+=1\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "c = list(zip(X_train, y_train))\n",
    "\n",
    "random.shuffle(c)\n",
    "\n",
    "X_train, y_train = zip(*c)\n",
    "\n",
    "X_train = list(X_train)\n",
    "y_train = list(y_train)\n",
    "\n",
    "y_train=list(np.array(y_train).reshape(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous créons notre bag of words pour les données d'entrainement "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function TextIOWrapper.close()>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f= open(\"dict_mot/stop_words_english.txt\",\"r\")\n",
    "stop_words_en=f.read().splitlines()\n",
    "f.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = {'Text':X_train}\n",
    "data = pd.DataFrame(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Amer Sports divests an industrial site in Rumi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Donations to universities The Annual General M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Los Angeles-based Pacific Office Properties Tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Copper , lead and nickel also dropped ... HBOS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>However , net sales in 2010 are seen to have g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1795</th>\n",
       "      <td>Ruukki Group calculates that it has lost EUR 4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1796</th>\n",
       "      <td>Finnair said that the cancellation of flights ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1797</th>\n",
       "      <td>The company 's scheduled traffic , measured in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1798</th>\n",
       "      <td>Also , a six-year historic analysis is provide...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1799</th>\n",
       "      <td>The companies signed the letter of intent for ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1800 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Text\n",
       "0     Amer Sports divests an industrial site in Rumi...\n",
       "1     Donations to universities The Annual General M...\n",
       "2     Los Angeles-based Pacific Office Properties Tr...\n",
       "3     Copper , lead and nickel also dropped ... HBOS...\n",
       "4     However , net sales in 2010 are seen to have g...\n",
       "...                                                 ...\n",
       "1795  Ruukki Group calculates that it has lost EUR 4...\n",
       "1796  Finnair said that the cancellation of flights ...\n",
       "1797  The company 's scheduled traffic , measured in...\n",
       "1798  Also , a six-year historic analysis is provide...\n",
       "1799  The companies signed the letter of intent for ...\n",
       "\n",
       "[1800 rows x 1 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'language'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-aa3deffb1ce5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mbag_of_words_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNLP\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdf_to_bow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstop_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstop_words_en\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlanguage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'en'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mTFIDF\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mbag_of_words_train\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\Travail\\Travail SXS\\PIE\\PIE\\Louis\\Natural_Language_Processing.py\u001b[0m in \u001b[0;36mdf_to_bow\u001b[1;34m(df, stop_words, language, TFIDF)\u001b[0m\n\u001b[0;32m    149\u001b[0m     (generally better performances)'''\n\u001b[0;32m    150\u001b[0m     \u001b[0mtext_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Text\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 151\u001b[1;33m     \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_tokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstop_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    152\u001b[0m     \u001b[0mcountvect\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_df\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.95\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_df\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m     \u001b[0mbow\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcountvect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\Travail\\Travail SXS\\PIE\\PIE\\Louis\\Natural_Language_Processing.py\u001b[0m in \u001b[0;36mget_tokenizer\u001b[1;34m(stop_words, language)\u001b[0m\n\u001b[0;32m    139\u001b[0m         \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFrenchStemTokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstop_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mlanguage\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'en'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m         \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLemmaTokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstop_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m     \u001b[1;32melse\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mLanguageError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Language not supported\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'language'"
     ]
    }
   ],
   "source": [
    "bag_of_words_train = NLP.df_to_bow(data,stop_words = stop_words_en,language = 'en',TFIDF = True)\n",
    "bag_of_words_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons maintenant tester differents modèles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.61944444, 0.57777778, 0.56388889, 0.55833333, 0.59166667])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gaussian_sentiment = GaussianNB()\n",
    "scores_gaussian = cross_val_score(gaussian_sentiment,bag_of_words_train,y_train,cv=5)\n",
    "scores_gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.34722222, 0.37222222, 0.33333333, 0.26111111, 0.34166667])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_sentiment = OneClassSVM(kernel = 'rbf',gamma=0.01,nu=0.01)\n",
    "scores_gaussian = cross_val_score(svm_sentiment,bag_of_words_train,y_train,cv=5,scoring='accuracy')\n",
    "scores_gaussian "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.75      , 0.74444444, 0.72777778, 0.75277778, 0.78888889])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd_sentiment = RandomForestClassifier(n_estimators=1000,criterion='entropy')\n",
    "scores_gaussian = cross_val_score(rnd_sentiment,bag_of_words_train,y_train,cv=5,scoring='accuracy')\n",
    "scores_gaussian "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 87  28  12]\n",
      " [  1 115  14]\n",
      " [  3  29  71]]\n",
      "[[ 89  19   6]\n",
      " [  5 100  10]\n",
      " [  5  36  90]]\n",
      "[[ 81  25  10]\n",
      " [  8 101   8]\n",
      " [  7  32  88]]\n",
      "[[86 28 10]\n",
      " [ 8 93 11]\n",
      " [ 2 34 88]]\n",
      "[[ 81  26  12]\n",
      " [  5 107  14]\n",
      " [ 10  22  83]]\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=5,shuffle=True)\n",
    "for train_index,test_index in kf.split(y_train):\n",
    "    X_train_fold, X_test_fold = bag_of_words_train.iloc[train_index], bag_of_words_train.iloc[test_index]\n",
    "    y_train_fold, y_test_fold = y_train[train_index], y_train[test_index]\n",
    "    rnd_sentiment = RandomForestClassifier(n_estimators=500,criterion='entropy')\n",
    "    rnd_sentiment.fit(X_train_fold,y_train_fold)\n",
    "    y_predict = rnd_sentiment.predict(X_test_fold)\n",
    "    print(confusion_matrix(y_test_fold,y_predict,labels=[-1, 0, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La plupart des erreurs sont des textes positifs/negatifs interprétés comme neutres, il faudrait se pencher sur cela peut être en rajoutant plus de données positives et négatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regardons un peu le résultat sur des tweets réels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authentification ok\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Author</th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>More than 170 prominent business leaders have ...</td>\n",
       "      <td>CNNBusiness</td>\n",
       "      <td>2021-01-05 15:05:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A group of major oil producers including Saudi...</td>\n",
       "      <td>CNNBusiness</td>\n",
       "      <td>2021-01-05 14:29:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Alibaba is shutting down its music streaming a...</td>\n",
       "      <td>CNNBusiness</td>\n",
       "      <td>2021-01-05 14:00:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>More than 170 prominent business leaders have ...</td>\n",
       "      <td>CNNBusiness</td>\n",
       "      <td>2021-01-05 00:34:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A second round of stimulus payments are on the...</td>\n",
       "      <td>CNNBusiness</td>\n",
       "      <td>2021-01-04 23:32:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Fiat Chrysler Automobiles and Groupe PSA, the ...</td>\n",
       "      <td>CNNBusiness</td>\n",
       "      <td>2021-01-04 22:33:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The results of Georgia's Senate runoffs will p...</td>\n",
       "      <td>CNNBusiness</td>\n",
       "      <td>2021-01-04 22:16:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Microsoft knows there's an Xbox shortage. It's...</td>\n",
       "      <td>CNNBusiness</td>\n",
       "      <td>2021-01-04 21:41:47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Haven, the health care company founded in 2018...</td>\n",
       "      <td>CNNBusiness</td>\n",
       "      <td>2021-01-04 20:29:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Chipotle is adding cauliflower rice to its men...</td>\n",
       "      <td>CNNBusiness</td>\n",
       "      <td>2021-01-04 20:00:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Janet Yellen, President-elect Joe Biden's pick...</td>\n",
       "      <td>CNNBusiness</td>\n",
       "      <td>2021-01-04 19:30:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>McDonald's is launching new sandwiches next mo...</td>\n",
       "      <td>CNNBusiness</td>\n",
       "      <td>2021-01-04 19:00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>The Dow tumbled more than 600 points around mi...</td>\n",
       "      <td>CNNBusiness</td>\n",
       "      <td>2021-01-04 17:18:41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Investors are gearing up for a turbocharged 20...</td>\n",
       "      <td>CNNBusiness</td>\n",
       "      <td>2021-01-04 17:08:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Google workers have formed the company's first...</td>\n",
       "      <td>CNNBusiness</td>\n",
       "      <td>2021-01-04 16:01:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Work-related deaths reached a 12-year high in ...</td>\n",
       "      <td>CNNBusiness</td>\n",
       "      <td>2021-01-04 13:01:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>A digitally created fake Queen Elizabeth II da...</td>\n",
       "      <td>CNNBusiness</td>\n",
       "      <td>2021-01-04 11:01:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>These robots can boogie down better than most ...</td>\n",
       "      <td>CNNBusiness</td>\n",
       "      <td>2021-01-04 09:01:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Prince Harry and Meghan, Duchess of Sussex, ha...</td>\n",
       "      <td>CNNBusiness</td>\n",
       "      <td>2021-01-04 08:01:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2020 was tough on many industries, but the aut...</td>\n",
       "      <td>CNNBusiness</td>\n",
       "      <td>2021-01-04 05:01:03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Text       Author  \\\n",
       "0   More than 170 prominent business leaders have ...  CNNBusiness   \n",
       "1   A group of major oil producers including Saudi...  CNNBusiness   \n",
       "2   Alibaba is shutting down its music streaming a...  CNNBusiness   \n",
       "3   More than 170 prominent business leaders have ...  CNNBusiness   \n",
       "4   A second round of stimulus payments are on the...  CNNBusiness   \n",
       "5   Fiat Chrysler Automobiles and Groupe PSA, the ...  CNNBusiness   \n",
       "6   The results of Georgia's Senate runoffs will p...  CNNBusiness   \n",
       "7   Microsoft knows there's an Xbox shortage. It's...  CNNBusiness   \n",
       "8   Haven, the health care company founded in 2018...  CNNBusiness   \n",
       "9   Chipotle is adding cauliflower rice to its men...  CNNBusiness   \n",
       "10  Janet Yellen, President-elect Joe Biden's pick...  CNNBusiness   \n",
       "11  McDonald's is launching new sandwiches next mo...  CNNBusiness   \n",
       "12  The Dow tumbled more than 600 points around mi...  CNNBusiness   \n",
       "13  Investors are gearing up for a turbocharged 20...  CNNBusiness   \n",
       "14  Google workers have formed the company's first...  CNNBusiness   \n",
       "15  Work-related deaths reached a 12-year high in ...  CNNBusiness   \n",
       "16  A digitally created fake Queen Elizabeth II da...  CNNBusiness   \n",
       "17  These robots can boogie down better than most ...  CNNBusiness   \n",
       "18  Prince Harry and Meghan, Duchess of Sussex, ha...  CNNBusiness   \n",
       "19  2020 was tough on many industries, but the aut...  CNNBusiness   \n",
       "\n",
       "                  Date  \n",
       "0  2021-01-05 15:05:03  \n",
       "1  2021-01-05 14:29:06  \n",
       "2  2021-01-05 14:00:14  \n",
       "3  2021-01-05 00:34:03  \n",
       "4  2021-01-04 23:32:03  \n",
       "5  2021-01-04 22:33:02  \n",
       "6  2021-01-04 22:16:03  \n",
       "7  2021-01-04 21:41:47  \n",
       "8  2021-01-04 20:29:07  \n",
       "9  2021-01-04 20:00:09  \n",
       "10 2021-01-04 19:30:06  \n",
       "11 2021-01-04 19:00:06  \n",
       "12 2021-01-04 17:18:41  \n",
       "13 2021-01-04 17:08:22  \n",
       "14 2021-01-04 16:01:07  \n",
       "15 2021-01-04 13:01:08  \n",
       "16 2021-01-04 11:01:05  \n",
       "17 2021-01-04 09:01:13  \n",
       "18 2021-01-04 08:01:02  \n",
       "19 2021-01-04 05:01:03  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data = TDF.search_author('CNNBusiness',20,remove_URL=True)\n",
    "Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'More than 170 prominent business leaders have signed a letter urging Congress to accept the Electoral College results that declared Joe Biden as the next President of the United States '"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Texte_1 = Data['Text'][0]\n",
    "Texte_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Haven, the health care company founded in 2018, is shutting down. The joint venture by Amazon, Berkshire Hathaway and JPMorgan Chase struggled to make inroads beyond its three partners. '"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Texte_2 = Data['Text'][8]\n",
    "Texte_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Janet Yellen, President-elect Joe Biden's pick for Treasury secretary, made more than $7 million in recent years by giving speeches to Wall Street banks, major corporations and industry groups. \""
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Texte_3 = Data['Text'][10]\n",
    "Texte_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = bag_of_words_train.columns\n",
    "validation_dataframe = pd.DataFrame(columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "words,BOW = NLP.bagofwords(Texte_1,'en', stop_words=stop_words_en, remove_non_words=False, stemming=False)\n",
    "for i in range(len(words)):\n",
    "    if words[i] in columns:\n",
    "        validation_dataframe.loc[0,words[i]] = BOW[i]\n",
    "\n",
    "\n",
    "words,BOW = NLP.bagofwords(Texte_2,'en', stop_words=stop_words_en, remove_non_words=False, stemming=False)\n",
    "ind = validation_dataframe.shape[0]\n",
    "for i in range(len(words)):\n",
    "    if words[i] in columns:\n",
    "        validation_dataframe.loc[ind,words[i]] = BOW[i]\n",
    "\n",
    "        \n",
    "words,BOW = NLP.bagofwords(Texte_3,'en', stop_words=stop_words_en, remove_non_words=False, stemming=False)\n",
    "ind = validation_dataframe.shape[0]\n",
    "for i in range(len(words)):\n",
    "    if words[i] in columns:\n",
    "        validation_dataframe.loc[ind,words[i]] = BOW[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job</th>\n",
       "      <th>raahe</th>\n",
       "      <th>reduced</th>\n",
       "      <th>steel</th>\n",
       "      <th>total</th>\n",
       "      <th>work</th>\n",
       "      <th>down</th>\n",
       "      <th>eur</th>\n",
       "      <th>group</th>\n",
       "      <th>loss</th>\n",
       "      <th>...</th>\n",
       "      <th>search</th>\n",
       "      <th>tata</th>\n",
       "      <th>perkonoja</th>\n",
       "      <th>considered</th>\n",
       "      <th>eero</th>\n",
       "      <th>gathered</th>\n",
       "      <th>sihvonen</th>\n",
       "      <th>taking</th>\n",
       "      <th>selects</th>\n",
       "      <th>transplace</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 4261 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   job  raahe  reduced  steel  total  work  down  eur  group  loss  ...  \\\n",
       "0    0      0        0      0      0     0     0    0      0     0  ...   \n",
       "1    0      0        0      0      0     0     1    0      0     0  ...   \n",
       "2    0      0        0      0      0     0     0    0      1     0  ...   \n",
       "\n",
       "   search  tata  perkonoja  considered  eero  gathered  sihvonen  taking  \\\n",
       "0       0     0          0           0     0         0         0       0   \n",
       "1       0     0          0           0     0         0         0       0   \n",
       "2       0     0          0           0     0         0         0       0   \n",
       "\n",
       "   selects  transplace  \n",
       "0        0           0  \n",
       "1        0           0  \n",
       "2        0           0  \n",
       "\n",
       "[3 rows x 4261 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_dataframe=validation_dataframe.fillna(0)\n",
    "validation_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(criterion='entropy', n_estimators=1000)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd_sentiment = RandomForestClassifier(n_estimators=1000,criterion='entropy')\n",
    "rnd_sentiment.fit(bag_of_words_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0., -1.,  0.])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd_sentiment.predict(validation_dataframe)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
